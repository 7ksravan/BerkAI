## Complex Feature Engineering Greatly Improves Used Car Price Prediction

![image](https://github.com/7ksravan/BerkAI/blob/main/images/capimage.jpg)

## Links
[Jupyter Notebook](https://github.com/7ksravan/BerkAI/blob/main/BerkAI_Customer_information_predicts_bank_marketing_success.ipynb)

## Summary
Spectacularly interesting results emerge after performing complex feature engineering on Used Car Prices dataset. Modeling with the newly engineered features shows an average of 20% reduction in RMSE error compared to modeling with the original features. There is a remarkable ~50% reduction in Random Forest Regressor's RMSE error. Linear Regression Model's RMSE error, training and test accuracy all show an impressive ~30% improvement. Random Forest Test Accuracy shot up to 90%! What also stands out is the fact that a whole different set of features emerge as the most important ones driving price prediction. Engineered Features like State Purchasing Parity (externally sourced) & Model numerical value stole the show compared to Odometer reading and Year of manufacturer. Quite some interesting work went into applying NLP to extract information from the 'messy' model feature. Also I referenced referencing external economic data to meaningfully use the states feature.

![image](https://github.com/7ksravan/BerkAI/blob/main/images/capplots.png)
    
## Project Hypothesis
My hypothesis is new features engineering have a significant impact on improving the predictive abilities of both regression and classification models. The idea is to start with a a huge data set with complex numerical and categorical features where the model performance is not good (<50%) to begin with. I will use the features in the dataset to engineer new features by applying a range of techniques from Natural Language Processing to numerical transformations. The baseline performance of multiple machine learning models on pre-feature-engineeded data will be compared against the model performance on the pre-feature-engineeded data to prove my hypothesis. 

## Motivation
Feature Engineering is the realm of creativity: literally sitting on the data and incubuating to generate thoughts. This will be an opportunity to improve and learn in the area of feature engineering, researching and trying diverse techniques. I thought of what can the VIN tell me and I learnt a lot taking the thought forward.

## Data
Dataset: USA Used Car Price prediction data set that contain ~420K records of user car sales price information.

Problem type: Regression

Target variable: PRICE

Features include:
1. Car make, model, manufacturer, YEAR of manufacture
2. Car specifications: ODOMETER reading, type, VIN, cyclinder, transmission, fuel type
3. Car status: title, condition
4. State and Region of sale
(upper case: numeric)

## ML Methodology
1. Data Preparation:
    * SIR curation (sorting, irrelevant column removal, renaming) for columns
    * DON curation (duplicates, outliers, nulls) for row
    * **50% of raw data was lost in removing duplicates, outliers, null**
    * special issues: data type conversion to integer year and odometer

2. Exploratory Data Analysis:
    * Descriptive Statistics: numerical and categorical features visualizations, distributions
    * Inferential Statistics: features impact on y, features relation among them

3. Metrics for Regression Evaluation:
   *  RMSE: Root Mean Square Error (RMSE): an interpretable error number
   *  Training Accuracy: helps reveal overfitting, especially for tree models
   *  Test Accuracy: in some ways, the real measure of model performance for regression models
  
4. Models used in this project:
     * Linear Regression
     * K-Nearest Neighbor
     * Decision Tree Regressor
     * Gradient Boost Regressor

## Feature Engineering Report

### Techniques used for Feature Engineering:
     * Natural Language Processing: to extract information from the 'irregular string' model feature
     * Automobile domain-specific python module: to extract information from VIN number
     * External data-referencing to transform features: US Bureau of labor data for State's purchasing parity to transform state feature
     * Target Encoding: on top of model values generated by NLP
     * Other: Binning, categorical-to-numerical mapping
     
### Feature Importance
Feature Importance determined by Linear Regression and Random Foreset Regression and also those by initial intuition are completely changed after the new features are engineered. The State's purchasing parity and Model numerical encoding emerge to be surprisingly important features. Also highly ranked was the out-of-box drive feature and the luxury brand feture I created. Original dataset features odometer and year were ranked medium/low by the models after the new engineered features were added.

![image](https://github.com/7ksravan/BerkAI/blob/main/images/capfeatplot.png)

### Engineered Features:
Below I discuss some features. For the rest, please refer to the Jupyter notebook.

1. model_numvalue: NLP functions applied to the model feature to extract text vectors. The top 100 text vectors were selected and new column created with those values if the corresponding rows contained them. This new column was then Target Encoded in reference to the Price. This feature turned out to be a strong predictor of the price, ranking in Top 5 important features and also showing a moderate positive correlation of 0.5 with the price.
   
2. VIN derived country-of-origin: The VIN field is NOT a junk field. Rather it is an entire repository of information by in itself. I used a domain-specific python module to extract the country of origin information from it. Although this feature ranked low in feature importance, I mention this as an example of complex feature engineering I did.
   
3. Car condition scale and Car title status scale are 2 features created by ordering the unique values and mapping onto a 1-5 numerical scale. Both emerged in the TOP 5 important features
   
4. State Regional Pricing Parity index gives the relative expensiveness of goods and services by US state. I experimented with it hoping it would help explain the price variation of similar profile car by state. Lo and Behold! It turned out to be among the TOP 5 beating Odometer & Year features.

5. Other features created by Categorical BINNING:
    * Luxury Brand: by binary binning of manufacturer as either luxurt or not
    * car type: binned 16 types into passenger, SUV, pickup/truck, other
    * paintcolor_priciness: binned 13 colors into low-price color, medium-price color, high-price color based on the proximity to price mean
   
6. Other categorical to numerical features:
    * cylinder numbers as text converted to number where missing imputed with mean
    * size scale: car sizes mapped to 1-5 numerical scale
   
![image](https://github.com/7ksravan/BerkAI/blob/main/images/capfeatimp.png)

## Future Work:

1. Further NLP processing of categorical features to extract valuable information
2. Extract more information from VIN: to create new features and also to get the missing values like manufacturer, make, model, other specification detals.
3. ~30% of the data was lost in duplicates, which is unavoidable. However, ~20% of the data is lost due to outliers and nulls, which may have been avoidable. These rows can be relooked to see if they can be retained and handled in an appropriate manner.
4. Dimensionality Reduction and Clustering of One-Hot encoded features created from categorical features to scrape further information from the categorical features without too much computational penalty.
